# NB(thxCode): The chosen rules of quantified models.
# 1. In general, recommend to use Q5_K_M, Q4_K_M, Q4_K_S, Q4_0, Q3_K_M, or Q3_K_S,
#    but for some sufficiently small models, like Embedding, it is recommended to use F16.
# 2. When packing with normal GitHub Action runner, the total size of all GGUF files cannot exceed 45GB,
#    the largest model is about 70B parameters.
#
include:
  #
  # LLaMA3.1
  #
  - name: "llama3.1"
    usage: "text-to-text"
    tag: "8b"
    repository: "mradermacher/Meta-Llama-3.1-8B-i1-GGUF"
    file: "Meta-Llama-3.1-8B.i1-Q5_K_M.gguf"
  - name: "llama3.1"
    usage: "text-to-text"
    tag: "8b-instruct"
    repository: "mradermacher/Meta-Llama-3.1-8B-Instruct-i1-GGUF"
    file: "Meta-Llama-3.1-8B-Instruct.i1-Q5_K_M.gguf"
  - name: "llama3.1"
    usage: "text-to-text"
    tag: "70b"
    repository: "mradermacher/Meta-Llama-3.1-70B-i1-GGUF"
    file: "Meta-Llama-3.1-70B.i1-Q4_K_M.gguf"
  - name: "llama3.1"
    usage: "text-to-text"
    tag: "70b-instruct"
    repository: "mradermacher/Meta-Llama-3.1-70B-Instruct-i1-GGUF"
    file: "Meta-Llama-3.1-70B-Instruct.i1-Q4_K_M.gguf"
  #
  # LLaMA3
  #
  - name: "llama3"
    usage: "text-to-text"
    tag: "8b"
    repository: "QuantFactory/Meta-Llama-3-8B-GGUF"
    file: "Meta-Llama-3-8B.Q5_K_M.gguf"
  - name: "llama3"
    usage: "text-to-text"
    tag: "8b-instruct"
    repository: "QuantFactory/Meta-Llama-3-8B-Instruct-GGUF"
    file: "Meta-Llama-3-8B-Instruct.Q5_K_M.gguf"
  - name: "llama3"
    usage: "text-to-text"
    tag: "70b"
    repository: "NousResearch/Meta-Llama-3-70B-GGUF"
    file: "Meta-Llama-3-70B-Q4_K_M.gguf"
  - name: "llama3"
    usage: "text-to-text"
    tag: "70b-instruct"
    repository: "QuantFactory/Meta-Llama-3-70B-Instruct-GGUF"
    file: "Meta-Llama-3-70B-Instruct.Q4_K_M.gguf"
  #
  # Gemma2
  #
  - name: "gemma2"
    usage: "text-to-text"
    tag: "2b"
    repository: "QuantFactory/gemma-2-2b-GGUF"
    file: "gemma-2-2b.Q5_K_M.gguf"
  - name: "gemma2"
    usage: "text-to-text"
    tag: "2b-instruct"
    repository: "QuantFactory/gemma-2-2b-it-GGUF"
    file: "gemma-2-2b-it.Q5_K_M.gguf"
  - name: "gemma2"
    usage: "text-to-text"
    tag: "9b"
    repository: "QuantFactory/gemma-2-9b-GGUF"
    file: "gemma-2-9b.Q5_K_M.gguf"
  - name: "gemma2"
    usage: "text-to-text"
    tag: "9b-instruct"
    repository: "QuantFactory/gemma-2-9b-it-GGUF"
    file: "gemma-2-9b-it.Q5_K_M.gguf"
  - name: "gemma2"
    usage: "text-to-text"
    tag: "27b"
    repository: "mradermacher/gemma-2-27b-i1-GGUF"
    file: "gemma-2-27b.i1-Q5_K_M.gguf"
  - name: "gemma2"
    usage: "text-to-text"
    tag: "27b-instruct"
    repository: "mradermacher/gemma-2-27b-it-i1-GGUF"
    file: "gemma-2-27b-it.i1-Q5_K_M.gguf"
  #
  # CodeGemma1.1
  #
  - name: "codegemma1.1"
    usage: "text-to-text"
    tag: "2b"
    repository: "mmnga/codegemma-1.1-2b-gguf"
    file: "codegemma-1.1-2b-Q5_K_M.gguf"
  - name: "codegemma1.1"
    usage: "text-to-text"
    tag: "7b-instruct"
    repository: "mmnga/codegemma-1.1-7b-it-gguf"
    file: "codegemma-1.1-7b-it-Q5_K_M.gguf"
  #
  #
  # Qwen2
  #
  - name: "qwen2"
    usage: "text-to-text"
    tag: "0.5b"
    repository: "QuantFactory/Qwen2-0.5B-GGUF"
    file: "Qwen2-0.5B.Q5_K_M.gguf"
  - name: "qwen2"
    usage: "text-to-text"
    tag: "0.5b-instruct"
    repository: "Qwen/Qwen2-0.5B-Instruct-GGUF"
    file: "qwen2-0_5b-instruct-q5_k_m.gguf"
  - name: "qwen2"
    usage: "text-to-text"
    tag: "1.5b"
    repository: "QuantFactory/Qwen2-1.5B-GGUF"
    file: "Qwen2-1.5B.Q5_K_M.gguf"
  - name: "qwen2"
    usage: "text-to-text"
    tag: "1.5b-instruct"
    repository: "Qwen/Qwen2-1.5B-Instruct-GGUF"
    file: "qwen2-1_5b-instruct-q5_k_m.gguf"
  - name: "qwen2"
    usage: "text-to-text"
    tag: "7b"
    repository: "QuantFactory/Qwen2-7B-GGUF"
    file: "Qwen2-7B.Q5_K_M.gguf"
  - name: "qwen2"
    usage: "text-to-text"
    tag: "7b-instruct"
    repository: "Qwen/Qwen2-7B-Instruct-GGUF"
    file: "qwen2-7b-instruct-q5_k_m.gguf"
  - name: "qwen2"
    usage: "text-to-text"
    tag: "72b"
    repository: "mradermacher/Qwen2-72B-GGUF"
    file: "Qwen2-72B.Q4_K_S.gguf"
  - name: "qwen2"
    usage: "text-to-text"
    tag: "72b-instruct"
    repository: "Qwen/Qwen2-72B-Instruct-GGUF"
    file: "qwen2-72b-instruct-q4_0.gguf"
  #
  # Qwen2-Math
  #
  - name: "qwen2-math"
    usage: "text-to-text"
    tag: "1.5b"
    repository: "QuantFactory/Qwen2-Math-1.5B-GGUF"
    file: "Qwen2-Math-1.5B.Q5_K_M.gguf"
  - name: "qwen2-math"
    usage: "text-to-text"
    tag: "1.5b-instruct"
    repository: "QuantFactory/Qwen2-Math-1.5B-Instruct-GGUF"
    file: "Qwen2-Math-1.5B-Instruct.Q5_K_M.gguf"
  - name: "qwen2-math"
    usage: "text-to-text"
    tag: "7b"
    repository: "QuantFactory/Qwen2-Math-7B-GGUF"
    file: "Qwen2-Math-7B.Q5_K_M.gguf"
  - name: "qwen2-math"
    usage: "text-to-text"
    tag: "7b-instruct"
    repository: "QuantFactory/Qwen2-Math-7B-Instruct-GGUF"
    file: "Qwen2-Math-7B-Instruct.Q5_K_M.gguf"
  #
  # CodeQwen1.5
  #
  - name: "codeqwen1.5"
    usage: "text-to-text"
    tag: "7b"
    repository: "Qwen/CodeQwen1.5-7B-Chat-GGUF"
    file: "codeqwen-1_5-7b-chat-q5_k_m.gguf"
  #
  # Mistral
  #
  - name: "mistral"
    usage: "text-to-text"
    tag: "7b-v0.1"
    repository: "QuantFactory/Mistral-7B-v0.1-GGUF"
    file: "Mistral-7B-v0.1.Q5_K_M.gguf"
  - name: "mistral"
    usage: "text-to-text"
    tag: "7b-instruct-v0.1"
    repository: "QuantFactory/Mistral-7B-Instruct-v0.1-GGUF"
    file: "Mistral-7B-Instruct-v0.1.Q5_K_M.gguf"
  - name: "mistral"
    usage: "text-to-text"
    tag: "7b-v0.2"
    repository: "QuantFactory/Mistral-7B-v0.2-hf-GGUF"
    file: "Mistral-7B-v0.2-hf.Q5_K_M.gguf"
  - name: "mistral"
    usage: "text-to-text"
    tag: "7b-instruct-v0.2"
    repository: "QuantFactory/Mistral-7B-Instruct-v0.2-GGUF"
    file: "Mistral-7B-Instruct-v0.2.Q5_0.gguf"
  - name: "mistral"
    usage: "text-to-text"
    tag: "7b-v0.3"
    repository: "QuantFactory/Mistral-7B-v0.3-GGUF"
    file: "Mistral-7B-v0.3.Q5_K_M.gguf"
  - name: "mistral"
    usage: "text-to-text"
    tag: "7b-instruct-v0.3"
    repository: "QuantFactory/Mistral-7B-Instruct-v0.3-GGUF"
    file: "Mistral-7B-Instruct-v0.3.Q5_K_M.gguf"
  #
  # Mistral-Nemo
  #
  - name: "mistral-nemo"
    usage: "text-to-text"
    tag: "base-2407" # 12B
    repository: "QuantFactory/Mistral-Nemo-Base-2407-GGUF"
    file: "Mistral-Nemo-Base-2407.Q5_K_M.gguf"
  - name: "mistral-nemo"
    usage: "text-to-text"
    tag: "instruct-2407" # 12B
    repository: "QuantFactory/Mistral-Nemo-Instruct-2407-GGUF"
    file: "Mistral-Nemo-Instruct-2407.Q5_K_M.gguf"
  #
  # Mixtral
  #
  - name: "mixtral"
    usage: "text-to-text"
    tag: "8x7b-v0.1" # 47B
    repository: "TheBloke/Mixtral-8x7B-v0.1-GGUF"
    file: "mixtral-8x7b-v0.1.Q5_K_M.gguf"
  - name: "mixtral"
    usage: "text-to-text"
    tag: "8x7b-instruct-v0.1" # 47B
    repository: "TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF"
    file: "mixtral-8x7b-instruct-v0.1.Q5_K_M.gguf"
  #
  # Mathstral
  #
  - name: "mathstral"
    usage: "text-to-text"
    tag: "7b-v0.1"
    repository: "MaziyarPanahi/mathstral-7B-v0.1-GGUF"
    file: "mathstral-7B-v0.1.Q5_K_M.gguf"
  #
  # Codestral
  #
  - name: "codestral"
    usage: "text-to-text"
    tag: "22b-v0.1"
    repository: "bartowski/Codestral-22B-v0.1-GGUF"
    file: "Codestral-22B-v0.1-Q5_K_M.gguf"
  #
  # GLM4
  #
  - name: "glm4"
    usage: "text-to-text"
    tag: "9b"
    repository: "legraphista/glm-4-9b-chat-GGUF"
    file: "glm-4-9b-chat.Q5_K.gguf"
  - name: "glm4"
    usage: "text-to-text"
    tag: "9b-1m"
    repository: "legraphista/glm-4-9b-chat-1m-GGUF"
    file: "glm-4-9b-chat-1m.Q5_K.gguf"
  #
  # Phi3
  #
  - name: "phi3"
    usage: "text-to-text"
    tag: "mini-4k-instruct" # 4B
    repository: "SanctumAI/Phi-3-mini-4k-instruct-GGUF"
    file: "phi-3-mini-4k-instruct.Q5_K_M.gguf"
  - name: "phi3"
    usage: "text-to-text"
    tag: "mini-128k-instruct" # 4B
    repository: "QuantFactory/Phi-3-mini-128k-instruct-GGUF"
    file: "Phi-3-mini-128k-instruct.Q5_K_M.gguf"
  - name: "phi3"
    usage: "text-to-text"
    tag: "medium-4k-instruct" # 14B
    repository: "bartowski/Phi-3-medium-4k-instruct-GGUF"
    file: "Phi-3-medium-4k-instruct-Q5_K_M.gguf"
  - name: "phi3"
    usage: "text-to-text"
    tag: "medium-128k-instruct" # 14B
    repository: "bartowski/Phi-3-medium-128k-instruct-GGUF"
    file: "Phi-3-medium-128k-instruct-Q5_K_M.gguf"
  #
  # MiniCPM-V2.6
  #
  - name: "minicpm-v2.6"
    usage: "image-to-text"
    tag: "qwen2-7b"
    repository: "openbmb/MiniCPM-V-2_6-gguf"
    file: "ggml-model-Q5_K_M.gguf"
    project_file: "mmproj-model-f16.gguf"
  #
  # MiniCPM-V2.5
  #
  - name: "minicpm-v2.5"
    usage: "image-to-text"
    tag: "llama3-8b"
    repository: "openbmb/MiniCPM-Llama3-V-2_5-gguf"
    file: "ggml-model-Q5_K_M.gguf"
    project_file: "mmproj-model-f16.gguf"
  #
  # LLaVA1.6
  #
  - name: "llava1.6"
    usage: "image-to-text"
    tag: "mistral-7b"
    repository: "cjpais/llava-1.6-mistral-7b-gguf"
    file: "llava-v1.6-mistral-7b.Q5_K_M.gguf"
    project_file: "mmproj-model-f16.gguf"
  - name: "llava1.6"
    usage: "image-to-text"
    tag: "vicuna-7b"
    repository: "cjpais/llava-v1.6-vicuna-7b-gguf"
    file: "llava-v1.6-vicuna-7b.Q5_K_M.gguf"
    project_file: "mmproj-model-f16.gguf"
  - name: "llava1.6"
    usage: "image-to-text"
    tag: "vicuna-13b"
    repository: "cjpais/llava-v1.6-vicuna-13b-gguf"
    file: "llava-v1.6-vicuna-13b.Q5_K_M.gguf"
    project_file: "mmproj-model-f16.gguf"
  #
  # LLaVA1.5
  #
  - name: "llava1.5"
    usage: "image-to-text-quantize"
    quantize_type: "Q5_K_M"
    tag: "phi3-mini-4k-instruct" # 4B
    repository: "xtuner/llava-phi-3-mini-gguf"
    file: "llava-phi-3-mini-f16.gguf"
    project_file: "llava-phi-3-mini-mmproj-f16.gguf"
  - name: "llava1.5"
    usage: "image-to-text"
    tag: "vicuna-7b"
    repository: "second-state/Llava-v1.5-7B-GGUF"
    file: "llava-v1.5-7b-Q5_K_M.gguf"
    project_file: "llava-v1.5-7b-mmproj-model-f16.gguf"
  - name: "llava1.5"
    usage: "image-to-text-quantize"
    quantize_type: "Q5_K_M"
    tag: "llama3-8b"
    repository: "xtuner/llava-llama-3-8b-v1_1-gguf"
    file: "llava-llama-3-8b-v1_1-f16.gguf"
    project_file: "llava-llama-3-8b-v1_1-mmproj-f16.gguf"
  - name: "llava1.5"
    usage: "image-to-text"
    tag: "vicuna-13b"
    repository: "PsiPi/liuhaotian_llava-v1.5-13b-GGUF"
    file: "llava-v1.5-13b-Q5_K_M.gguf"
    project_file: "mmproj-model-f16.gguf"
  #
  # BGE
  #
  - name: "bge"
    usage: "embedding"
    tag: "small-zh-v1.5"
    repository: "CompendiumLabs/bge-small-zh-v1.5-gguf"
    file: "bge-small-zh-v1.5-f16.gguf"
  - name: "bge"
    usage: "embedding"
    tag: "small-en-v1.5"
    repository: "CompendiumLabs/bge-small-en-v1.5-gguf"
    file: "bge-small-en-v1.5-f16.gguf"
  - name: "bge"
    usage: "embedding"
    tag: "base-zh-v1.5"
    repository: "CompendiumLabs/bge-base-zh-v1.5-gguf"
    file: "bge-base-zh-v1.5-f16.gguf"
  - name: "bge"
    usage: "embedding"
    tag: "base-en-v1.5"
    repository: "CompendiumLabs/bge-base-en-v1.5-gguf"
    file: "bge-base-en-v1.5-f16.gguf"
  - name: "bge"
    usage: "embedding"
    tag: "large-zh-v1.5"
    repository: "CompendiumLabs/bge-large-zh-v1.5-gguf"
    file: "bge-large-zh-v1.5-f16.gguf"
  - name: "bge"
    usage: "embedding"
    tag: "large-en-v1.5"
    repository: "CompendiumLabs/bge-large-en-v1.5-gguf"
    file: "bge-large-en-v1.5-f16.gguf"
  - name: "bge"
    usage: "embedding"
    tag: "m3"
    repository: "vonjack/bge-m3-gguf"
    file: "bge-m3-f16.gguf"
    context_size: 8194
  #
  # GTE
  #
  - name: "gte"
    usage: "embedding"
    tag: "small"
    repository: "ChristianAzinn/gte-small-gguf"
    file: "gte-small_fp16.gguf"
  - name: "gte"
    usage: "embedding"
    tag: "base"
    repository: "ChristianAzinn/gte-base-gguf"
    file: "gte-base_fp16.gguf"
  - name: "gte"
    usage: "embedding"
    tag: "large"
    repository: "ChristianAzinn/gte-large-gguf"
    file: "gte-large_fp16.gguf"
  - name: "gte"
    usage: "embedding"
    tag: "qwen1.5-7b-instruct"
    repository: "gaianet/gte-Qwen1.5-7B-instruct-GGUF"
    file: "gte-Qwen1.5-7B-instruct-Q5_K_M.gguf"
    context_size: 32768
  - name: "gte"
    usage: "embedding"
    tag: "qwen2-1.5b-instruct"
    repository: "second-state/gte-Qwen2-1.5B-instruct-GGUF"
    file: "gte-Qwen2-1.5B-instruct-Q5_K_M.gguf"
    context_size: 131072
  - name: "gte"
    usage: "embedding"
    tag: "qwen2-7b-instruct"
    repository: "RichardErkhov/Alibaba-NLP_-_gte-Qwen2-7B-instruct-gguf"
    file: "gte-Qwen2-7B-instruct.Q5_K_M.gguf"
    context_size: 131072
