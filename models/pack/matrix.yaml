# NB(thxCode): The chosen rules of quantified models.
# 1. use Q5_K_M, Q4_K_M, Q4_K_S, Q4_0, IQ4_XS, Q3_K_M, or Q3_K_S.
# 2. GGUF file size cannot exceed 50GB.
#
include:
  #
  # LLaMA3.1
  #
  - name: "llama3.1"
    usage: "text-to-text"
    tag: "8b"
    repository: "mradermacher/Meta-Llama-3.1-8B-i1-GGUF"
    file: "Meta-Llama-3.1-8B.i1-Q5_K_M.gguf"
  - name: "llama3.1"
    usage: "text-to-text"
    tag: "8b-instruct"
    repository: "mradermacher/Meta-Llama-3.1-8B-Instruct-i1-GGUF"
    file: "Meta-Llama-3.1-8B-Instruct.i1-Q5_K_M.gguf"
  - name: "llama3.1"
    usage: "text-to-text"
    tag: "70b"
    repository: "mradermacher/Meta-Llama-3.1-70B-i1-GGUF"
    file: "Meta-Llama-3.1-70B.i1-Q4_K_M.gguf"
  - name: "llama3.1"
    usage: "text-to-text"
    tag: "70b-instruct"
    repository: "mradermacher/Meta-Llama-3.1-70B-Instruct-i1-GGUF"
    file: "Meta-Llama-3.1-70B-Instruct.i1-Q4_K_M.gguf"
  #
  # Gemma2
  #
  - name: "gemma2"
    usage: "text-to-text"
    tag: "2b"
    repository: "QuantFactory/gemma-2-2b-GGUF"
    file: "gemma-2-2b.Q5_K_M.gguf"
  - name: "gemma2"
    usage: "text-to-text"
    tag: "2b-instruct"
    repository: "QuantFactory/gemma-2-2b-it-GGUF"
    file: "gemma-2-2b-it.Q5_K_M.gguf"
  - name: "gemma2"
    usage: "text-to-text"
    tag: "9b"
    repository: "QuantFactory/gemma-2-9b-GGUF"
    file: "gemma-2-9b.Q5_K_M.gguf"
  - name: "gemma2"
    usage: "text-to-text"
    tag: "9b-instruct"
    repository: "QuantFactory/gemma-2-9b-it-GGUF"
    file: "gemma-2-9b-it.Q5_K_M.gguf"
  - name: "gemma2"
    usage: "text-to-text"
    tag: "27b"
    repository: "mradermacher/gemma-2-27b-i1-GGUF"
    file: "gemma-2-27b.i1-Q5_K_M.gguf"
  - name: "gemma2"
    usage: "text-to-text"
    tag: "27b-instruct"
    repository: "mradermacher/gemma-2-27b-it-i1-GGUF"
    file: "gemma-2-27b-it.i1-Q5_K_M.gguf"
  #
  #
  # Qwen2
  #
  - name: "qwen2"
    usage: "text-to-text"
    tag: "0.5b"
    repository: "QuantFactory/Qwen2-0.5B-GGUF"
    file: "Qwen2-0.5B.Q5_K_M.gguf"
  - name: "qwen2"
    usage: "text-to-text"
    tag: "0.5b-instruct"
    repository: "Qwen/Qwen2-0.5B-Instruct-GGUF"
    file: "qwen2-0_5b-instruct-q5_k_m.gguf"
  - name: "qwen2"
    usage: "text-to-text"
    tag: "1.5b"
    repository: "QuantFactory/Qwen2-1.5B-GGUF"
    file: "Qwen2-1.5B.Q5_K_M.gguf"
  - name: "qwen2"
    usage: "text-to-text"
    tag: "1.5b-instruct"
    repository: "Qwen/Qwen2-1.5B-Instruct-GGUF"
    file: "qwen2-1_5b-instruct-q5_k_m.gguf"
  - name: "qwen2"
    usage: "text-to-text"
    tag: "7b"
    repository: "QuantFactory/Qwen2-7B-GGUF"
    file: "Qwen2-7B.Q5_K_M.gguf"
  - name: "qwen2"
    usage: "text-to-text"
    tag: "7b-instruct"
    repository: "Qwen/Qwen2-7B-Instruct-GGUF"
    file: "qwen2-7b-instruct-q5_k_m.gguf"
  - name: "qwen2"
    usage: "text-to-text"
    tag: "math-1.5b"
    repository: "QuantFactory/Qwen2-Math-1.5B-GGUF"
    file: "Qwen2-Math-1.5B.Q5_K_M.gguf"
  - name: "qwen2"
    usage: "text-to-text"
    tag: "math-1.5b-instruct"
    repository: "QuantFactory/Qwen2-Math-1.5B-Instruct-GGUF"
    file: "Qwen2-Math-1.5B-Instruct.Q5_K_M.gguf"
  - name: "qwen2"
    usage: "text-to-text"
    tag: "math-7b"
    repository: "QuantFactory/Qwen2-Math-7B-GGUF"
    file: "Qwen2-Math-7B.Q5_K_M.gguf"
  - name: "qwen2"
    usage: "text-to-text"
    tag: "math-7b-instruct"
    repository: "QuantFactory/Qwen2-Math-7B-Instruct-GGUF"
    file: "Qwen2-Math-7B-Instruct.Q5_K_M.gguf"
  - name: "qwen2"
    usage: "text-to-text"
    tag: "72b"
    repository: "mradermacher/Qwen2-72B-GGUF"
    file: "Qwen2-72B.Q4_K_M.gguf"
  - name: "qwen2"
    usage: "text-to-text"
    tag: "72b-instruct"
    repository: "Qwen/Qwen2-72B-Instruct-GGUF"
    file: "qwen2-72b-instruct-q4_k_m.gguf"
  #
  # Mistral
  #
  - name: "mistral"
    usage: "text-to-text"
    tag: "7b-v0.1"
    repository: "QuantFactory/Mistral-7B-v0.1-GGUF"
    file: "Mistral-7B-v0.1.Q5_K_M.gguf"
  - name: "mistral"
    usage: "text-to-text"
    tag: "7b-instruct-v0.1"
    repository: "QuantFactory/Mistral-7B-Instruct-v0.1-GGUF"
    file: "Mistral-7B-Instruct-v0.1.Q5_K_M.gguf"
  - name: "mistral"
    usage: "text-to-text"
    tag: "7b-v0.2"
    repository: "QuantFactory/Mistral-7B-v0.2-hf-GGUF"
    file: "Mistral-7B-v0.2-hf.Q5_K_M.gguf"
  - name: "mistral"
    usage: "text-to-text"
    tag: "7b-instruct-v0.2"
    repository: "QuantFactory/Mistral-7B-Instruct-v0.2-GGUF"
    file: "Mistral-7B-Instruct-v0.2.Q5_0.gguf"
  - name: "mistral"
    usage: "text-to-text"
    tag: "7b-v0.3"
    repository: "QuantFactory/Mistral-7B-v0.3-GGUF"
    file: "Mistral-7B-v0.3.Q5_K_M.gguf"
  - name: "mistral"
    usage: "text-to-text"
    tag: "7b-instruct-v0.3"
    repository: "QuantFactory/Mistral-7B-Instruct-v0.3-GGUF"
    file: "Mistral-7B-Instruct-v0.3.Q5_K_M.gguf"
  - name: "mistral"
    usage: "text-to-text"
    tag: "nemo-base-2407" # 12B
    repository: "QuantFactory/Mistral-Nemo-Base-2407-GGUF"
    file: "Mistral-Nemo-Base-2407.Q5_K_M.gguf"
  - name: "mistral"
    usage: "text-to-text"
    tag: "nemo-instruct-2407" # 12B
    repository: "QuantFactory/Mistral-Nemo-Instruct-2407-GGUF"
    file: "Mistral-Nemo-Instruct-2407.Q5_K_M.gguf"
  - name: "mistral"
    usage: "text-to-text"
    tag: "mixtral-8x7b-v0.1" # 47B
    repository: "TheBloke/Mixtral-8x7B-v0.1-GGUF"
    file: "mixtral-8x7b-v0.1.Q5_K_M.gguf"
  - name: "mistral"
    usage: "text-to-text"
    tag: "mixtral-8x7b-instruct-v0.1" # 47B
    repository: "TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF"
    file: "mixtral-8x7b-instruct-v0.1.Q5_K_M.gguf"
  #
  # Phi3
  #
  - name: "phi3"
    usage: "text-to-text"
    tag: "mini-4k-instruct" # 4B
    repository: "SanctumAI/Phi-3-mini-4k-instruct-GGUF"
    file: "phi-3-mini-4k-instruct.Q5_K_M.gguf"
  - name: "phi3"
    usage: "text-to-text"
    tag: "mini-128k-instruct" # 4B
    repository: "QuantFactory/Phi-3-mini-128k-instruct-GGUF"
    file: "Phi-3-mini-128k-instruct.Q5_K_M.gguf"
  - name: "phi3"
    usage: "text-to-text"
    tag: "medium-4k-instruct" # 14B
    repository: "bartowski/Phi-3-medium-4k-instruct-GGUF"
    file: "Phi-3-medium-4k-instruct-Q5_K_M.gguf"
  - name: "phi3"
    usage: "text-to-text"
    tag: "medium-128k-instruct" # 14B
    repository: "bartowski/Phi-3-medium-128k-instruct-GGUF"
    file: "Phi-3-medium-128k-instruct-Q5_K_M.gguf"
  #
  # MiniCPM-V2.6
  #
  - name: "minicpm-v2.6"
    usage: "image-to-text"
    tag: "qwen2-7b"
    repository: "openbmb/MiniCPM-V-2_6-gguf"
    file: "ggml-model-Q5_K_M.gguf"
    project_file: "mmproj-model-f16.gguf"
  #
  # MiniCPM-V2.5
  #
  - name: "minicpm-v2.5"
    usage: "image-to-text"
    tag: "llama3-8b"
    repository: "openbmb/MiniCPM-Llama3-V-2_5-gguf"
    file: "ggml-model-Q5_K_M.gguf"
    project_file: "mmproj-model-f16.gguf"
  #
  # LLaVA1.6
  #
  - name: "llava1.6"
    usage: "image-to-text"
    tag: "mistral-7b"
    repository: "cjpais/llava-1.6-mistral-7b-gguf"
    file: "llava-v1.6-mistral-7b.Q5_K_M.gguf"
    project_file: "mmproj-model-f16.gguf"
  - name: "llava1.6"
    usage: "image-to-text"
    tag: "vicuna-7b"
    repository: "cjpais/llava-v1.6-vicuna-7b-gguf"
    file: "llava-v1.6-vicuna-7b.Q5_K_M.gguf"
    project_file: "mmproj-model-f16.gguf"
  - name: "llava1.6"
    usage: "image-to-text"
    tag: "vicuna-13b"
    repository: "cjpais/llava-v1.6-vicuna-13b-gguf"
    file: "llava-v1.6-vicuna-13b.Q5_K_M.gguf"
    project_file: "mmproj-model-f16.gguf"
  #
  # LLaVA1.5
  #
  - name: "llava1.5"
    usage: "image-to-text-quantize"
    quantize_type: "Q5_K_M"
    tag: "phi3-mini-4k-instruct" # 4B
    repository: "xtuner/llava-phi-3-mini-gguf"
    file: "llava-phi-3-mini-f16.gguf"
    project_file: "llava-phi-3-mini-mmproj-f16.gguf"
  - name: "llava1.5"
    usage: "image-to-text"
    tag: "vicuna-7b"
    repository: "second-state/Llava-v1.5-7B-GGUF"
    file: "llava-v1.5-7b-Q5_K_M.gguf"
    project_file: "llava-v1.5-7b-mmproj-model-f16.gguf"
  - name: "llava1.5"
    usage: "image-to-text-quantize"
    quantize_type: "Q5_K_M"
    tag: "llama3-8b"
    repository: "xtuner/llava-llama-3-8b-v1_1-gguf"
    file: "llava-llama-3-8b-v1_1-f16.gguf"
    project_file: "llava-llama-3-8b-v1_1-mmproj-f16.gguf"
  - name: "llava1.5"
    usage: "image-to-text"
    tag: "vicuna-13b"
    repository: "PsiPi/liuhaotian_llava-v1.5-13b-GGUF"
    file: "llava-v1.5-13b-Q5_K_M.gguf"
    project_file: "mmproj-model-f16.gguf"
